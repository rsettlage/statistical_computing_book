{
 "cells": [
  {
   "source": [
    "## Topics in Model Performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Underfitting vs. Overfitting\n",
    "\n",
    "Most folks have by now heard of underfitting and overfitting a model. Simpler models should be preferred but not at the cost of accuracy. An overfit model, on the other hand, may not generalize well on new data. We can measure how well a model fits the data using the $R^2$ metric which measures the proportion of explained variance.\n",
    "\n",
    "If we use the example of linear regression and start with a first order regression to explain the data, we may find that the data may not be adequately captured. We may have to incrementally increase the complexity of the model by increasing the order of the polynomial. Past a certain point, however, the model starts overfitting to the data. What this means is that the model simply used its representational power to memorize the data and will perform poorly on new data that is fed into the model.\n",
    "\n",
    "We want a model that has found that balance between being underfit and overfit, this trade-off is often referred to as the bias-variance trade-off. Bias is the error in the data resulting from its inability to accomodate the data. The model does not have the representational power to capture all the variations and patterns in the data. Variance is the error resulting from the sensitivity of the model to the data which usually results a complex model. Regularization is often used for this reason to reduce the complexity in a regression (or neural network) by minimizing the number of coefficients. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Measures for Predictive Performance\n",
    "\n",
    "Accuracy of the model can be measured by \n",
    "\n",
    "#### 1. Cross-validation \n",
    "\n",
    "Here we divide the data into non-overlapping subsets and perform training and validation on the different subsets. Depending on how we perform this cross-validation, it can be called K-fold cross-validation or leave-one-out cross-validation (LOOCV). In K-fold cross validation we divide the data into 'K' folds or subsets, perform training of the model on k-1 folds while the model performance is assessed on the 1 fold that was left. We iteratively select each fold to be the test fold while the others become the training folds.\n",
    "\n",
    "\n",
    "\n",
    "![Image from the scikit-learn page for K-fold cross validation](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png) \n",
    "*K-fold Cross-validation from the scikit-learn page *\n",
    "\n",
    "If the number of folds is equal to the number of data points, we have leave-one-out cross-validation.\n",
    "\n",
    "#### 2. Information criteria\n",
    "\n",
    "*Reference* [Predictive metrics presentation from Liberty Mutual](https://www.casact.org/education/rpm/2016/presentations/PM-LM-4-Tevet.pdf)\n",
    "\n",
    "A number of ideas that are firmly rooted in Information theory help us to quantify how well a model performs. \n",
    "\n",
    "1. Log-likelihood and deviance\n",
    "\n",
    "2. Akaike Information Criterion (AIC)\n",
    "\n",
    "3. Widely Applicable Information Criterion (WAIC)\n",
    "\n",
    "4. Bayesian Information Criterion (BIC)\n",
    "\n",
    "#### Log-likelihood and Deviance\n",
    "\n",
    "These terms are used to measure the error in our model with regards to the data that the model is trying to fit. Most folks are familiar with the Mean Squared Error (MSE) given by \n",
    "\n",
    "MSE = $\\sum_1^n (y_{true} - y_{predicted})^2 / n$\n",
    "\n",
    "While this is a perfectly acceptably way of measuring error, another way to measure the performance of a model is using the log-likelihood function.\n",
    "\n",
    "Log Likelihood = $\\sum_1^n log p(y_i | \\theta)$\n",
    "\n",
    "Note that the log likelihood function $p(y_i | \\theta)$ takes values from 0 for no fit to 1 for a perfectly fit model.\n",
    "\n",
    "If the likelihood function is a Normal, the log-likelihood is proportional to the MSE. Deviance is simply -2 times the log-likelihood\n",
    "\n",
    "Deviance = -2 $\\sum_1^n log p(y_i | \\theta)$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Entropy and KL Divergence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Model Averaging"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Ergodicity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### EVALUATION\n",
    "\n",
    "1. Underfitting is bad because\n",
    "\n",
    "    a. It cannot capture complex behavior and will have inherent error (C)\n",
    "\n",
    "    b. The predicted value is always less than the true value\n",
    "\n",
    "2. Overfitting is bad because \n",
    "\n",
    "    a. The model that is overfit will learn noise (C)\n",
    "\n",
    "    b. The model is too big\n",
    "\n",
    "3. Variance of a model is related to \n",
    "\n",
    "    a. A model's ability to adapt its parameters to training data\n",
    "    \n",
    "    b. The sensitivity of the model to the inputs\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}